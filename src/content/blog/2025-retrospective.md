---
title: '2025년 회고 - 백엔드 개발자의 한 해'
description: 'ASP에서 MSA로, 온프레미스에서 EKS로. 레거시 시스템 현대화와 함께 성장한 한 해를 돌아봅니다.'
pubDate: 2025-12-31
tags: ['Retrospective']
---

## 1년을 돌아보며

커머스 백엔드 개발자로 일한 지 1년이 됐다. 입사할 때 알았던 건, 이 회사가 ASP와 .NET으로 돌아가던 시스템을 Java + MSA로 전환 중이라는 것과 내가 이 작업을 담당하게 됐다는 것이다. 처음엔 괜찮을 거라 생각했는데, 실제로는 예상과 달랐다.

비즈니스는 멈추지 않는다. 레거시 전환하는 동안에도 신규 기능 개발 요청이 들어오고, 이벤트가 있고, 제휴몰 개편이 있고, 카테고리 리뉴얼이 있어서 이걸 다 병렬로 진행해야 한다. 1년 동안 뭘 했는지, 무엇을 배웠는지 돌아보고 싶어서 정리했다.

## 숫자로 보는 1년

크고 작은 27개의 프로덕트 작업을 했다. 비즈니스 기능이 17개, 이벤트가 7개, 제휴몰 개편이 3개로, 평균적으로 한 달에 2개 이상의 작업을 병렬로 진행한 셈이다. 그리고 IDC에서 EKS로 인프라를 전환하는 작업도 장기 프로젝트로 진행해서 올해 안에 완료했다.

Java 11에서 21로 버전을 올리는 작업도 상시로 했다. Virtual Thread, Record 같은 새로운 기능을 조금씩 적용했다. 요구사항 협의부터 개발, 테스트, 배포까지 전 과정을 담당했다. Full Cycle이라고 하면 좋게 들리지만, 실제로는 각 단계마다 집중해야 할 것들이 많다는 뜻이다.

## 점진적 전환을 경험하다

### 레거시 전환의 현실

책에서 읽었던 Strangler Fig Pattern을 실제로 경험했다. 레거시 시스템을 한 번에 바꾸는 게 아니라 조금씩 새로운 시스템으로 교체하는 패턴이다. 이론은 명확하다. 새로운 기능은 새 시스템에 만들고, 기존 기능은 필요할 때 옮기며, 레거시와 신규가 공존하면서 점진적으로 전환한다.

근데 현실에서는 레거시 코드를 이해하는 게 먼저다. ASP와 .NET으로 작성된 코드를 읽어야 하는데 문서도 없고 개발했던 당사자도 없어서, 코드를 읽으면서 "이게 왜 이렇게 돼있지?"라는 생각을 하게 된다. 신규 시스템은 Java + Spring Boot + MSA로 만들지만, MSA라고 해서 완벽하게 분리된 건 아니다. 여전히 레거시 DB를 공유하고 API 호출도 레거시와 신규가 섞여있다.

가장 어려웠던 건, "이 기능을 어디에 만들어야 하는가"를 판단하는 것이다. 신규 시스템에 만들면 좋지만, 레거시 의존성이 너무 크면 레거시를 먼저 고쳐야 한다. 비즈니스 요구사항을 협의할 때도 이걸 고려해야 한다.

경험을 통해 알게 된 건, 점진적 전환이 "천천히 편하게 하는 것"이 아니라는 점이다. 두 시스템을 동시에 이해하고, 유지보수하고, 개발해야 한다. 그래도 비즈니스를 멈출 수 없는 상황에서 조금씩 나아가는 게 현실적인 방법이라는 걸 알게 됐다.

처음 MSA로 분리한 서비스가 프로덕션에 안정적으로 올라갔을 때의 안도감은 아직도 기억난다. 되긴 되는구나. 길이 보이기 시작했다.

### IDC에서 AWS EKS로

인프라를 IDC에서 AWS EKS로 옮기는 작업을 올해 완료했는데, 전환하려고 했던 핵심 이유는 가용성과 확장성 문제를 해결하기 위해서다.

먼저 확장성 문제부터 겪었는데, 커머스 특성상 트래픽 변동이 커서 이벤트 기간에는 평소의 3배가 들어오기도 한다. 물리 서버는 증설하려면 서버를 주문하고 배송받고 랙에 올리고 네트워크를 설정하는 과정을 거쳐야 해서 짧게는 며칠, 길게는 몇 주가 걸린다. "3월에 이벤트 있으니까 2월에 서버 증설해야겠다"는 식으로 미리 준비해야 하는데, 그러면 평소에는 유휴 자원이 생기고 피크 트래픽 기준으로 서버를 구성하면 평소에는 자원을 제대로 활용하지 못하는 문제가 있다.

가용성도 고민이었는데, 물리 서버 한 대가 문제가 생기면 수동으로 대응해야 하고 서버를 재시작하거나 트래픽을 다른 서버로 옮기는 작업을 사람이 직접 해야 하니까 자동화된 장애 복구가 어렵다. 새벽에 장애 알람이 오면 접속해서 직접 확인하고 조치해야 하는 구조였다.

EKS는 확장성 측면에서 Auto Scaling이 가능해서 트래픽이 늘면 Pod이 자동으로 늘고 줄면 자동으로 줄어들며 몇 분 안에 확장이 완료된다. 가용성 측면에서도 Pod 하나가 죽으면 자동으로 재시작되고, Health Check가 실패하면 트래픽을 다른 Pod으로 자동으로 분산하며, Blue-Green 배포와 Canary 배포로 무중단 배포도 가능하다.

물론 EKS가 좋다는 건 알겠는데, 전환 과정 자체가 쉽지는 않았다. 다행히 DevOps팀의 도움을 받을 수 있어서 VPC 구성, 서브넷 설계, 보안 그룹 설정, Load Balancer 구성 같은 인프라와 네트워크 설정은 DevOps팀이 담당했고, 내가 맡은 건 애플리케이션 레벨 작업이었다. 기존 서비스를 멈출 수 없어서 IDC에도 서비스가 돌고 EKS에도 서비스가 돌아가는 상태로 병렬 운영을 하면서 트래픽을 조금씩 EKS로 옮기고 테스트하고, 문제가 생기면 롤백하는 방식으로 진행하는데 이 롤백 전략을 미리 세워두는 게 중요했다.

야간에 트래픽 전환 작업을 할 때는 손에 땀이 났는데, Grafana 대시보드를 켜놓고 지표를 보면서 조금씩 트래픽을 올리는데 에러율이 조금이라도 튀면 바로 롤백 버튼에 손이 갔다. 한 번은 새벽 3시에 갑자기 레이턴시가 튀어서 급하게 롤백한 적이 있는데, 원인을 분석해보니 예상 못한 쿼리 패턴 때문이었고 그날 이후로 전환 전에 체크해야 할 항목이 하나 더 늘었다.

결국 프로덕션 전환까지 완료했는데, 개발 환경과 스테이징 환경을 먼저 전환하고 프로덕션은 트래픽 일부를 EKS로 보내면서 안정성을 검증하는 단계를 거쳤다. 비즈니스 일정과 조율하면서 진행하다 보니 예상보다 오래 걸렸는데, 이벤트가 있으면 미루고 제휴몰 오픈이 있으면 또 미루고 하다 보니 처음 예상했던 것보다 몇 달이 더 걸렸다.

### Java 11에서 21로, Spring Boot 2에서 3으로

Java 21과 Spring Boot 3으로 버전을 올리는 작업을 상시로 하고 있다. 두 가지 버전이 혼용되면서 유지보수가 어려워졌기 때문이다.

입사했을 때부터 이미 두 가지 버전이 공존하고 있었다. 레거시 시스템은 Java 11 + Spring Boot 2였고, 일부 신규 서비스는 Java 21 + Spring Boot 3으로 개발되어 있어서 처음부터 두 버전을 동시에 유지보수해야 하는 상황이었다. 같은 기능을 구현하는데 설정 방법이 달랐다. 예를 들어 Validation을 설정할 때, Spring Boot 2는 javax.validation을 쓰고 Spring Boot 3은 jakarta.validation을 쓴다.

코어 라이브러리를 공유하는데 버전별로 다른 설정이 필요해서 복잡했다. "이 설정은 어느 버전용이지?" 하면서 확인해야 했고, 공통 유틸리티를 만들 때도 양쪽 버전을 다 지원하도록 조건부로 작성하거나 아예 별도로 만들어야 했다. 새로운 팀원이 온다면 두 가지 방식을 다 알려줘야 한다. 유지보수 비용이 계속 늘어나는 구조였다.

그래서 통일하기로 했다. Java 21에 추가된 기능들도 활용하고 싶었고, Spring Boot 3의 개선사항도 쓰고 싶었다. Virtual Thread가 가장 인상적이었다. 기존에는 Thread Pool 크기 때문에 동시 처리 수가 제한됐다. Virtual Thread를 쓰면 수만 개의 Thread를 가볍게 만들 수 있다.

Record도 유용했다. DTO를 Record로 만들면 코드가 간결해지고 Immutable하며 equals/hashCode도 자동으로 생성된다. Pattern Matching도 좋았는데, instanceof 체크하고 캐스팅하는 보일러플레이트가 줄어든다.

Spring Boot 3으로 올리면서 겪은 변경사항도 있다. 가장 큰 건 패키지 변경이었다. javax 패키지가 jakarta로 바뀌었다. 다행히 IntelliJ에서 마이그레이션을 도와주는 기능이 있었다. 설정 파일도 바뀌었다. deprecated된 설정을 찾아서 제거하거나 대체했다. 의존성 라이브러리 중 일부는 Spring Boot 3과 호환되지 않아서 버전을 올리거나 다른 라이브러리로 교체했다.

근데 모든 코드를 한 번에 바꿀 수는 없어서, 점진적으로 전환하고 있다. 레거시 코드는 Java 11 + Spring Boot 2로 돌아가고 신규 코드는 Java 21 + Spring Boot 3을 쓴다. 이것도 공존 중이다.

## 장애가 나면

점진적 전환의 또 다른 어려움은 장애가 발생했을 때 원인을 찾는 것이다. 특히 배치 작업에서 문제가 생기면 추적이 복잡했다.

Jenkins에서 스케줄링된 배치 작업이 실행되면 ASP 레거시 시스템을 거쳐서 MSA 신규 서비스를 호출하고, 최종적으로 MSSQL 프로시저까지 이어지는 흐름이다. 이 과정 중 어디서 문제가 발생했는지 파악하는 게 매우 어려웠고 시간도 오래 걸렸다. 각 레이어마다 로그 형식이 다르고 저장 위치도 달라서 전체 흐름을 재구성하는 것 자체가 하나의 작업이었다.

Jenkins 배치 로그를 먼저 확인하고, ASP 시스템의 IIS 로그를 찾아서 요청이 제대로 전달됐는지 확인하고, MSA 서비스의 애플리케이션 로그에서 API 호출 내역을 추적하고, MSSQL 프로시저는 직접 실행해보거나 Profiler로 쿼리를 확인해야 했다. 프로시저 내부에서 에러가 발생해도 예외 처리가 제대로 안 돼있으면 상위 레이어에서는 정상 응답처럼 보이는 경우도 있었다.

ASP의 VBScript, Java의 Spring Boot, MSSQL의 T-SQL 프로시저를 모두 이해해야 했다. 레거시와 신규가 섞여 있는 구조에서 의존성을 추적하는 것 자체가 복잡한 아키텍처 분석 작업이었다. "이 API는 어디서 호출되지?" "이 프로시저는 누가 실행하지?" 하면서 코드를 역추적하는 시간이 문제 해결 시간보다 더 길 때도 있었다.

이 문제를 해결하기 위해 여러 방법을 시도했다. 문서화가 가장 먼저 도움이 됐다. 주요 배치 작업별로 "Jenkins Job 이름 → ASP 파일 경로 → MSA API 엔드포인트 → 호출하는 프로시저 이름"까지 흐름도를 그렸다. 각 레이어의 로그 파일 경로와 로그 레벨, 에러 발생 시 확인해야 할 포인트를 정리했다. 완벽한 문서는 아니지만 새로운 사람이 와도 참고할 수 있는 수준은 됐다.

중앙집중식 로깅 시스템도 검토 중이다. ELK Stack이나 Grafana Loki 같은 도구로 모든 레이어의 로그를 한곳에 모으면 검색이 훨씬 쉬울 것 같다. 다만 레거시 ASP의 IIS 로그까지 수집하려면 추가 작업이 필요해서 당장은 어렵다. MSA와 배치 작업 로그부터 점진적으로 통합하려고 한다.

## 병렬 작업, 생산성의 고민

### Context Switching의 비용

가장 힘든 건 Context Switching이다. 극단적인 예시지만, 이런 날도 있다. 오전에는 카테고리 개편 작업을 한다. 카테고리 구조를 이해하고, 새로운 속성을 추가하고, 검색 로직을 수정한다. 머릿속이 카테고리로 가득 차있다. 오후에는 출석체크 이벤트 작업을 한다. Redis를 어떻게 쓸지, Kafka로 이벤트를 어떻게 발행할지. 머릿속을 비우고 다시 채워야 한다. 저녁에는 제휴몰 API 리뷰가 온다. 베네피아몰 스펙을 다시 읽어야 한다. 외부 시스템 연동 규격을 확인한다.

이렇게 하루에 3개 컨텍스트를 전환하는 날도 있다. 머릿속이 지워지고 다시 로드되는 느낌이다. 연구에 따르면, Context Switching에는 평균 23분이 걸린다고 한다. 작업을 멈추고, 다른 작업으로 전환하고, 다시 집중하기까지. 하루에 3번 전환하면, 1시간 이상이 사라진다. 체감상으로도 맞는 것 같다. 아침에 카테고리 작업하다가, 갑자기 출석체크 이슈가 발생하면? "어디까지 했더라?" 하면서 코드를 다시 읽는다.

비즈니스 요구사항이 동시에 들어오는 상황은 피할 수 없는 현실이었다. 그럼 어떻게 대응해야 하는가?

### 머리를 비우는 방법

나는 Jira를 외부 메모리처럼 쓰기 시작했다. 작업을 전환해야 할 때 현재 상태를 Jira 티켓에 상세하게 남겨두는데, "여기까지 했고, 다음은 이거 해야 함"이라고 적어두면 나중에 돌아왔을 때 흐름을 다시 파악하는 시간을 크게 줄일 수 있다. 심지어 코드 위치까지 남긴다. "ProductService.java 157줄, TODO 주석 확인"처럼 구체적으로 적어두면 IDE를 열자마자 바로 그 지점으로 갈 수 있어서, 코드를 다시 찾아 헤매는 시간이 사라진다.

처음에는 이렇게까지 해야 하나 싶었는데, 막상 해보니 효과가 확실했다. 핵심은 내 머리를 비우는 것이다. 모든 걸 기억하려고 하면 인지 부하가 쌓이고, 정작 중요한 것에 집중하기 어려워진다. 기억해야 할 것들을 외부에 맡기고 나니, 머릿속에 여유 공간이 생겼다.

### Time Boxing

칸반 방법론에서 WIP(Work In Progress) 제한이라는 개념이 있는데, 동시에 진행하는 작업 수를 제한해서 한 번에 하나씩만 처리하면 Context Switching이 줄어든다는 이론이다. 이상적으로는 완전히 맞는 말이지만, 현실에서는 적용하기 어려웠다.

비즈니스 요구사항은 한 번에 하나씩 순서대로 들어오지 않고 여러 개가 동시에 들어오는데, 이벤트는 날짜가 정해져 있어서 미룰 수 없고, 제휴몰은 상대방 일정에 맞춰야 하고, 카테고리 개편은 기획 리뷰가 길어지면서 다른 작업과 겹치게 된다. 내가 "하나씩만 하겠습니다"라고 할 수 있는 상황이 아니었고, 비즈니스는 내 워크플로우에 맞춰서 기다려주지 않는다.

그래서 WIP 제한 대신 Time Boxing 방식을 선택했는데, 오전 2시간은 카테고리 작업만 하고 오후 2시간은 출석체크 작업만 하는 식으로 정해진 시간 블록에 특정 작업만 집중하는 것이다. 그 시간이 끝나면 작업 상태를 Jira에 저장하고 다음 작업으로 넘어가는데, 이렇게 하면 하루에 여러 작업을 진행하면서도 각 작업에 최소한의 집중 시간을 확보할 수 있다.

물론 완벽한 방법은 아니어서 때로는 시간 안에 못 끝내기도 하고, 긴급 이슈가 발생해서 계획이 틀어지기도 하지만 아무 계획 없이 들어오는 대로 처리하는 것보다는 훨씬 낫다는 걸 체감했다.

Time Boxing을 하면서 부수적으로 알게 된 건 내가 작업 시간을 심하게 과소평가한다는 점인데, "이거 1시간이면 되겠지" 했던 작업이 3시간 걸리는 일이 반복되면서 예상과 현실의 차이를 매번 기록하고 다음 추정에 반영하려고 노력하고 있다.

### Git 브랜치, 혼자서는 단순하게

병렬 작업을 하면서 Git 브랜치 전략을 고민했는데, Git Flow나 GitHub Flow처럼 각 작업마다 feature 브랜치를 만들면 작업이 격리되고 롤백도 쉬울 것 같아서 여러 작업이 동시에 진행되니까 브랜치로 나누는 게 맞는 것처럼 보였다.

근데 혼자 작업하다 보니 브랜치 전략이 오히려 오버헤드였는데, 브랜치 충돌 걱정도 없고 코드 리뷰 프로세스도 필요 없고 PR을 만들어서 내가 나한테 승인하는 것도 이상했다. 여러 브랜치에 작업이 흩어지면 오히려 전체 상황 파악이 어려워서 "지금 이벤트 작업 어디까지 했더라?" 하고 브랜치를 왔다갔다 하는 게 비효율적이었고, 결국 단일 브랜치(develop)에서 작업하는 게 빠르고 효율적이라는 결론에 도달했다.

물론 장점도 있고 단점도 있는데, 빠른 배포가 가능하고 워크플로우가 간단하며 전체 작업 상황을 한눈에 볼 수 있다는 장점이 있는 반면에 커밋 히스토리가 여러 작업으로 섞이고 한 기능에 문제가 생겨도 부분 롤백이 어렵다는 단점이 있다. 완벽한 방법은 아니지만 1인 개발 상황에서는 현실적인 선택이었다.

대신 나름의 규칙을 만들었는데, 커밋 메시지에 [카테고리] 속성 필터 추가, [이벤트/출석체크] Redis 캐시 적용 같은 식으로 prefix를 붙여서 어떤 작업인지 한눈에 알 수 있게 하고 있다. 나중에 팀원이 생기면 그때는 제대로 된 브랜치 전략을 도입해야겠다는 생각이다.

## 커머스 도메인

### 도메인별 프로덕트

1년 동안 캐릭터 전문관, 디자인문구관, 다꾸존(다이어리 꾸미기) 같은 여러 도메인을 경험했는데, 각 도메인마다 핵심이 되는 포인트가 달랐다. 캐릭터는 IP 관리가 핵심이고, 디자인문구는 속성 기반 필터링이 핵심이고, 다꾸는 취향 기반 큐레이션이 핵심이어서 각 비즈니스마다 다른 특성을 이해하고 그에 맞는 기능을 설계해야 했다.

카테고리 개편 작업은 생각보다 복잡했는데, 기존 카테고리 구조를 깨지 않으면서 새로운 속성 기반 검색을 추가해야 하는 제약이 있었다. 검색 결과 페이지(SRP), 상품 목록 페이지(PLP), 상품 상세 페이지(PDP)는 각각 다른 최적화 포인트가 있어서 검색은 속도에 집중하고, 목록은 필터 정확도에 집중하고, 상세는 데이터 정확성에 집중하는 식으로 접근했다.

HOME, SRP, PLP 개편 후 지표를 보면 효과가 확실히 있었는데, 홈 방문 대비 PLP 진입률이 1% 미만에서 7-9%로 약 7-8배 상승해서 사용자가 카테고리를 더 적극적으로 클릭하게 됐다. PLP에서 작품 클릭 CVR도 60%에서 74%로 14%p 상승했고, 고객 당 작품 클릭수도 4-5건에서 7-8건으로 증가했는데 속성 기반 필터링이 사용자의 탐색 경험을 개선한 것으로 보인다.

도메인마다 다른 어려움이 있었지만 그만큼 얻은 것도 많았는데, 커머스라는 게 단순히 물건을 사고파는 게 아니라 카테고리 구조, 검색 로직, 상품 속성, 큐레이션까지 생각보다 복잡한 비즈니스라는 걸 온몸으로 체감한 1년이었다.

### 이벤트의 특성

이벤트는 비즈니스 기능 개발과는 또 다른 성격을 가지고 있는데, 독서생활, 키링 통합전, 상반기 결산, 다이어리 룸, 24주년 같은 이벤트들을 진행하면서 각각의 특성이 다르다는 걸 체감했다. 가장 큰 특징은 일정이 정해져 있다는 점인데, 3월호 이벤트는 반드시 3월에 열어야 하고 24주년 이벤트는 그 날짜에 맞춰야 하니까 기능 개발처럼 일정을 조율하거나 늦출 수 있는 여지가 없다.

트래픽 패턴도 일반 서비스와 다른데, 이벤트가 시작되면 동시 접속자가 한꺼번에 몰리고, 특히 출석체크 같은 경우 하루에 한 번만 할 수 있다 보니 오전 10시에 트래픽이 집중되는 특성이 있어서 DB 부하를 줄이기 위해 Redis BitMap을 활용해서 해결했다. 이벤트가 끝나면 트래픽은 얼마나 들어왔는지, 에러율은 어땠는지, 응답 속도는 괜찮았는지 돌아보면서 다음 이벤트에 반영할 점을 정리한다.

출석체크는 일회성 이벤트가 아니라 상시로 운영되는 이벤트라서 안정성이 특히 중요했는데, 장애가 나면 유저가 출석을 못 하게 되고 보상 처리도 복잡해지기 때문에 다른 이벤트보다 더 신경을 많이 썼다. 이벤트를 만들 때 항상 고민하게 되는 건 코드를 일회용으로 만들 것인지, 재사용 가능하게 만들 것인지 하는 문제인데, 매번 처음부터 새로 만들면 시간이 오래 걸리고 패턴화해서 만들어두면 빠르게 대응할 수 있지만 너무 일반화하면 오히려 복잡해져서 이해하기 어려워진다. 적당한 수준의 추상화를 찾는 게 쉽지 않다.

### 제휴몰 연동

베네피아몰, 현대 이지웰, SSG 같은 제휴몰은 자사 서비스와는 성격이 많이 달랐는데, 외부 시스템과 연동해야 하다 보니 API 스펙이 이미 정해져 있어서 우리가 그에 맞춰야 한다. 스펙 문서를 읽고 이해한 다음, 테스트 환경에서 API를 호출해보고, 문제가 없으면 실제 연동하는 과정을 거치는데 이 과정이 예상보다 까다로웠다.

API 스펙이 바뀔 때 사전 공지가 오면 다행인데, 때로는 갑자기 바뀌어서 에러가 터진 다음에야 알게 되는 경우도 있어서 빠르게 대응해야 한다. 장애 대응도 복잡한데, 문제가 생겼을 때 우리 쪽 문제인지 상대방 쪽 문제인지를 로그를 보고 판단해야 하고, 상대방 쪽 문제로 보이면 확인 요청을 보내고 응답을 기다려야 하니 시간이 오래 걸린다. 정산 로직은 더 복잡한데, 주문은 우리 시스템에서 발생하지만 정산은 제휴몰 기준으로 해야 하다 보니 데이터를 맞춰야 하고 차이가 나면 그 원인을 파악해서 조정해야 한다.

베네피아몰 개편, 현대 이지웰 차세대 개편, SSG 개편을 하면서 느낀 건 각 제휴사마다 스펙이 다르다는 것인데, 공통화할 수 있는 부분이 있긴 하지만 각 제휴사만의 특수한 요구사항도 있어서 완전히 추상화하기는 어렵다.

제휴몰 작업을 하면서 가장 크게 배운 건 외부 시스템과의 통합이 생각보다 훨씬 어렵다는 것인데, API를 호출하는 것 자체는 간단하지만 네트워크 지연, 타임아웃, 상대방 서버 장애 같은 다양한 예외 상황에 대응하는 게 어렵다. Timeout은 얼마로 설정할지, Retry는 몇 번까지 할지, 상대방 서버가 죽었을 때 Circuit Breaker는 어떻게 동작시킬지 같은 것들을 다 고민해야 한다.

## 협업과 커뮤니케이션

### 요구사항 협의

기획자, 디자이너와 협업하면서 느낀 건 비즈니스 요구사항을 기술로 번역하는 게 백엔드 개발자의 중요한 역할 중 하나라는 것이다. "사용자가 상품을 좋아요 누르면 위시리스트에 추가됩니다"라는 기획을 받으면 "API를 호출하고 Redis에 캐시하고 DB에 저장합니다"라는 기술적 구현으로 바꿔야 한다.

"이건 안 됩니다"라고 말해야 할 때가 있는데, 기술적으로 불가능하거나 시간이 너무 오래 걸리는 경우가 그렇다. 근데 "안 됩니다"로 끝내면 안 된다는 걸 배웠는데, 왜 안 되는지를 설명하고 대신 이렇게 하면 된다는 대안을 제시해야 상대방도 납득하고 다음 스텝으로 넘어갈 수 있다.

트레이드오프를 명확히 설명하려고 노력했는데, "A 방법은 빠르게 구현할 수 있지만 정확하지 않고, B 방법은 시간이 더 걸리지만 정확합니다. 비즈니스적으로 어느 게 더 중요한가요?"라는 식으로 선택지를 제공해서 비즈니스 측에서 판단하고 선택하도록 했다.

때로는 개발 중간에 기획이 바뀌거나 요구사항이 추가되는데, 쉽지 않지만 이런 상황에서 유연하게 대응하는 것도 필요한 능력이라는 걸 배웠다. 다만 변경 비용은 투명하게 설명하는데, "지금 이걸 바꾸면 일정이 며칠 더 소요됩니다"라고 명확하게 말해야 서로 기대치를 맞출 수 있다.

### Full Cycle 참여

요구사항부터 모니터링까지 전 과정을 다 담당하다 보니 한 기능을 처음부터 끝까지 경험하게 되는데, 요구사항을 받으면 어떤 API가 필요하고 어떤 테이블이 필요하고 어떤 외부 시스템과 연동해야 하는지 설계하고, 그다음 코드를 작성하고 테스트하고 배포까지 한다.

QA가 따로 없어서 테스트도 직접 하는데, API를 호출해보고 화면을 클릭해보고 예외 상황을 일부러 만들어보면서 "사용자가 이렇게 하면 어떻게 되지?"라는 질문을 던지면서 여러 시나리오를 테스트한다. 테스트 커버리지를 높이려고 노력하지만 시간에 쫓기면 핵심 기능 위주로만 테스트하게 되는 게 현실이다.

배포는 Teamcity를 통해 진행하는데, 처음에는 롤링 배포 방식이었다. 서버를 하나씩 내리고 새 버전을 올리고 다음 서버로 넘어가는 방식인데, 배포 중에 일부 요청이 실패할 수 있고 문제가 생기면 롤백하는 데 시간이 걸린다는 단점이 있었다.

EKS로 전환하면서 블루그린 배포로 개선했는데, Green 환경에 먼저 배포하고 테스트한 후 트래픽을 Green으로 넘기는 방식이라 문제가 생기면 바로 Blue로 트래픽을 돌릴 수 있어서 롤백이 빠르고 다운타임도 없다. 모니터링은 Grafana로 응답 속도, 에러율, 트래픽을 확인하고 이상이 감지되면 Slack으로 알람이 오게 설정해뒀다.

장애가 나면 로그를 보고 원인을 찾아서 롤백할지 핫픽스할지 빠르게 판단해야 하는데, 이런 판단을 하려면 인프라, 네트워크, DB 등 다양한 영역을 어느 정도 알아야 한다. 솔직히 깊이는 부족하지만 넓게 알고 있어서 최소한 어느 방향으로 파고들어야 하는지는 감이 온다.

Full Cycle을 경험하면서 얻은 가장 큰 건 전체 흐름을 보는 눈이 생겼다는 것인데, 코드를 작성할 때 "이 코드가 프로덕션에서 어떻게 동작할까?"를 자연스럽게 생각하게 됐다.

### 측정과 판단

피드백을 줄 동료가 없다 보니 어떤 메트릭이 중요한지, 어느 정도가 괜찮은 수준인지를 스스로 정해야 했다.

응답 속도는 평균이 아니라 P95를 기준으로 보는데, 평균이 100ms여도 P95가 3초라면 상위 5%의 사용자는 느리다고 느끼기 때문이다. Grafana에서 P95와 P99를 항상 확인하고 있고, P95가 200ms를 넘어가면 원인을 파악해서 개선한다.

에러율은 0.1%를 목표로 하고 있는데, 완벽한 0%는 현실적으로 불가능하다. 네트워크 타임아웃이나 외부 API 장애, 사용자의 예외적인 입력 같은 이유로 에러는 항상 발생하기 마련인데, 0.1%를 넘어가면 뭔가 잘못됐다는 신호로 보고 로그를 확인해서 패턴을 찾는다.

이런 기준들이 정답인지는 솔직히 확신하기 어렵지만, 적어도 "느낌"이 아니라 "숫자"로 판단할 수 있게 됐다는 게 중요하다고 생각한다. 숫자가 있어야 현재 상태를 파악할 수 있고, 파악할 수 있어야 개선도 할 수 있으니까.

## 성장에 대한 생각

1년 동안 혼자서 많은 걸 해오면서, 시니어 개발자가 된다는 게 뭔지 생각해보게 됐다. 예전엔 복잡한 코드를 잘 짜는 사람이라고 생각했는데, 지금은 좀 다르게 본다.

시스템 전체를 이해하고, 장애가 났을 때 어디를 봐야 하는지 아는 사람. 트래픽이 10배 늘어나면 어디가 먼저 터질지 예측할 수 있는 사람. 새로운 기능을 추가할 때 기존 시스템에 어떤 영향을 미칠지 판단할 수 있는 사람. 이런 역량은 코드 몇 줄 잘 짜는 것과는 결이 다르다. "어떻게 구현할까"보다 "이 문제를 왜 풀어야 하는지", "여러 방법 중 왜 이걸 선택해야 하는지"를 판단하는 게 더 어렵고 더 중요하다는 걸 느꼈다.

올해 경험한 것들, 레거시 전환, EKS 마이그레이션, 장애 추적, 병렬 작업 관리 같은 것들이 결국 이런 역량을 쌓는 과정이었던 것 같다. 코드를 잘 짜는 것보다 올바른 문제를 정의하고 방향을 설정하는 사람이 되는 것. 아직 갈 길이 멀지만, 적어도 어떤 방향으로 가야 하는지는 조금 보이기 시작했다.

## AI와 함께 일하기

올해 가장 큰 변화 중 하나는 AI 도구를 적극적으로 쓰기 시작한 것이다. 복잡한 레거시 코드 분석, 다이어그램 자동 생성, 반복적인 CRUD 코드 작성까지. Entity 만들고, Repository 만들고, Service 만들고, Controller 만들고. 패턴이 정해져 있는 작업들은 AI가 빠르게 뽑아주니까, 그 시간에 비즈니스 로직을 더 고민할 수 있었다.

그런데 AI가 못하는 것도 분명히 있다. "이 API는 동시에 몇 명이 호출할까?", "트래픽이 몰리면 어디가 병목이 될까?", "이 데이터는 캐시해도 되는 성격인가?" 같은 질문들은 우리 서비스의 맥락을 알아야 답할 수 있다. 쿼리 튜닝도 마찬가지인데, AI가 "인덱스를 이렇게 걸어라"라고 제안할 순 있지만 실제 프로덕션 데이터 분포와 쿼리 패턴을 모르면 그 제안이 맞는지 알 수 없다.

AI와 함께 일하면서 가장 크게 느낀 건, 개발자의 역할 자체가 바뀌고 있다는 것이다. 실제로 내 하루를 돌아보면, 코드를 직접 타이핑하는 시간보다 AI에게 컨텍스트를 설명하고, 결과물을 검토하고, 방향을 수정하는 시간이 더 길어졌다. 처음에는 "내가 코드를 안 짜면 개발자인가?" 하는 생각도 들었는데, 곰곰이 생각해보니 코드를 타이핑하는 건 수단이지 목적이 아니었다. 목적은 좋은 시스템을 만드는 거고, AI가 그 과정을 빠르게 해준다면 그건 좋은 일이다.

그렇다고 기본기가 필요 없어지는 건 아니다. AI가 만들어준 코드에서 트랜잭션 경계가 잘못 잡혀있거나 N+1 문제가 숨어있는 걸 발견한 적이 몇 번 있는데, 내가 그 영역을 이해하고 있어서 잡아낸 거지 몰랐으면 그냥 넘어갔을 것이다. 오히려 시스템은 점점 복잡해지고 있다. MSA, 쿠버네티스, 이벤트 드리븐 아키텍처, 분산 트랜잭션. 코드 짜는 건 AI가 도와줄 수 있지만, 이 복잡한 시스템들이 어떻게 맞물려 돌아가는지 이해하고 운영하는 건 여전히 사람의 영역이다.

결국 개발자의 역할이 "작성자"에서 "감독"으로 이동하고 있다는 느낌이다. 코드를 직접 쓰기보다는 큰 그림을 그리고, AI가 만든 조각들을 검토하고, 전체가 잘 맞물려 돌아가는지 확인하는 역할. 이 흐름을 거스를 수는 없을 것 같고, AI를 도구로 쓰면서 더 빠르게 더 좋은 시스템을 만드는 사람이 되는 게 목표다.

## 돌아보며

1년이라는 시간이 길다면 길고 짧다면 짧지만, 돌아보면 생각보다 정말 많은 걸 경험했다는 생각이 든다.

병렬 작업을 관리하는 나름의 체계를 만든 건 잘한 선택이었다. Time Boxing으로 시간을 나누고 Jira에 작업 상태를 기록하고 AI를 보조 도구로 활용하면서 혼자서도 여러 작업을 병렬로 진행할 수 있는 워크플로우를 구축했는데, 완벽한 시스템은 아니지만 아무것도 없이 그때그때 대응하는 것보다는 확실히 효율적이었다. 점진적 전환을 직접 경험한 것도 큰 배움이었는데, 책에서만 읽었던 Strangler Fig Pattern을 실전에서 체득하면서 이론과 현실 사이의 간극을 메우는 경험을 했다.

반면에 성급한 일반화의 대가도 톡톡히 치렀다. 초기에 "재사용 가능한 이벤트 프레임워크"를 만들겠다고 추상화를 여러 단계로 나누고 설정으로 모든 걸 제어할 수 있게 설계했는데, 실제로 두 번째 이벤트를 만들 때 오히려 복잡해서 손이 안 갔다. 결국 그 프레임워크는 포기하고 단순하게 다시 만들었는데, 공통 부분만 유틸로 빼고 나머지는 각 이벤트마다 따로 구현하니 더 빠르고 이해하기도 쉬웠다. 재사용성보다 단순함이 더 중요한 경우가 많다는 걸 몸소 체험한 셈이다.

문서화는 양면이 있었다. README에 "왜 이렇게 만들었는지"를 적어두고 온보딩 문서를 미리 만들어둔 건 3개월 전에 만든 코드를 수정할 때 큰 도움이 됐다. 근데 문서화를 미뤘다가 고생한 적도 있는데, "이게 뭐 하는 API였더라?" 하면서 코드를 한참 들여다보느라 시간을 낭비했다. 나중에 하려고 미루면 결국 안 하게 되거나 그때 가서 더 힘들어진다는 걸 뼈저리게 느꼈고, 그 이후로는 바로바로 쓰려고 노력하고 있다.

기술 부채가 계속 쌓인 건 아쉬운 부분이다. "나중에 고쳐야지" 했던 것들이 점점 늘어나고 테스트 커버리지도 낮은 상태로 유지됐다. MSA로 전환하면서 서비스 경계를 나누는데 "이 경계가 정말 맞는 건가?" 싶을 때가 있었는데, 피드백을 줄 사람이 없다 보니 내가 놓친 걸 지적받을 기회가 없다는 게 솔직히 불안했다. 그래도 "느낌"이 아니라 "숫자"로 판단하는 습관은 생겼다. P95 응답속도가 몇 ms인지, 에러율이 몇 퍼센트인지 같은 기준이 있어야 상태를 판단할 수 있고, 판단할 수 있어야 개선도 할 수 있다는 걸 배웠다.

## 앞으로

2026년에는 새로운 회사로 이직이 확정됐다. 1년 동안 레거시 전환, EKS 마이그레이션, 다양한 프로덕트 개발을 경험하면서 성장했지만, 이제는 새로운 환경에서 또 다른 도전을 해보고 싶다는 생각이 들었다. 혼자서 많은 것을 해왔지만, 더 큰 팀에서 동료들과 함께 일하면서 배우고 싶은 마음도 컸다.

EKS 전환은 올해 안에 프로덕션까지 완료했는데, 중간에 이벤트나 제휴몰 일정 때문에 여러 번 미뤄지면서 예상보다 오래 걸렸지만 결국 해냈다는 게 뿌듯하다. 떠나기 전에 마무리할 수 있어서 다행이다.

새로운 곳에서는 Kubernetes를 표면적으로만 쓰는 수준이 아니라 제대로 깊이 있게 공부해보고 싶고, 성능 최적화 쪽도 더 파고들고 싶다. 무엇보다 당장 해야 할 일에만 쫓기지 않고 중요한 일에 집중할 수 있는 여유를 갖고 싶은데, 새로운 환경에서는 조금 달라지지 않을까 기대하고 있다.

## 마치며

매일 조금씩이라도 나아지고 있다는 건 느끼는데, 레거시 전환처럼 한 번에 확 바꿀 수 없는 것들도 점진적으로 나아가고 있다는 걸 이 회고를 쓰면서 다시 한번 확인했다. 1년 전의 나와 지금의 나를 비교하면 확실히 성장했고, 그 성장이 눈에 보이는 결과물로도 남아있다.

이 회사에서의 1년은 끝나지만, 개발자로서의 여정은 계속된다. 새로운 환경에서 또 어떤 도전이 기다리고 있을지 모르겠지만, 이번 1년 동안 배운 것들이 분명 도움이 될 거라고 생각한다. 1년 뒤에 이 글을 다시 읽을 때 "그때 이직 잘했네"라고 쓸 수 있으면 좋겠다.
